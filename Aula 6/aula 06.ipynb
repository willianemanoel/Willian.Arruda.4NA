{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Instalação das bibliotecas (execute apenas se não estiverem instaladas)\n",
    "# !pip install gensim matplotlib scikit-learn pandas numpy spacy plotly\n",
    "\n",
    "# Importações básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download de recursos NLTK (se necessário)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto 1: Este filme é incrível, adorei a atuação do protagonista\n",
      "Texto 2: A direção de fotografia é espetacular e o roteiro é envolvente\n",
      "Texto 3: Péssimo filme, desperdicei meu tempo assistindo isso\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo - críticas de filmes (simplificadas)\n",
    "textos = [\n",
    "    \"Este filme é incrível, adorei a atuação do protagonista\",\n",
    "    \"A direção de fotografia é espetacular e o roteiro é envolvente\",\n",
    "    \"Péssimo filme, desperdicei meu tempo assistindo isso\",\n",
    "    \"Os atores são talentosos mas o roteiro é fraco\",\n",
    "    \"Cinematografia belíssima, recomendo assistir no cinema\",\n",
    "    \"Não gostei da história, personagens mal desenvolvidos\",\n",
    "    \"A trilha sonora combina perfeitamente com as cenas\",\n",
    "    \"Filme entediante, previsível do início ao fim\",\n",
    "    \"Os efeitos especiais são impressionantes, tecnologia de ponta\",\n",
    "    \"História emocionante, chorei no final do filme\"\n",
    "]\n",
    "\n",
    "# Verificando os dados\n",
    "for i, texto in enumerate(textos[:3]):  # Mostrando apenas os 3 primeiros\n",
    "    print(f\"Texto {i+1}: {texto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remover caracteres especiais e números\n",
    "    texto = re.sub(r'[^a-záàâãéèêíïóôõöúçñ ]', '', texto)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(texto)\n",
    "\n",
    "    # Remover stopwords (opcional, dependendo da aplicação)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Aplicar pré-processamento a todos os textos\n",
    "textos_preprocessados = [preprocessar_texto(texto) for texto in textos]\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"Exemplo de texto original:\")\n",
    "print(textos[0])\n",
    "print(\"\\nDepois do pré-processamento:\")\n",
    "print(textos_preprocessados[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parâmetros do modelo\n",
    "vector_size = 100    # Dimensionalidade dos vetores\n",
    "window = 5           # Tamanho da janela de contexto\n",
    "min_count = 1        # Frequência mínima das palavras\n",
    "workers = 4          # Número de threads para treinamento\n",
    "sg = 1               # Modelo Skip-gram (1) ou CBOW (0)\n",
    "\n",
    "# Treinar o modelo\n",
    "model = Word2Vec(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg\n",
    ")\n",
    "\n",
    "print(f\"Modelo treinado com {len(model.wv.key_to_index)} palavras no vocabulário\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar modelo spaCy (pequeno)\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")  # Para português\n",
    "    # Alternativa para inglês: nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Exemplo de texto\n",
    "    texto = \"O banco está cheio de dinheiro. Eu sentei no banco da praça.\"\n",
    "\n",
    "    # Processar o texto\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # Examinar embeddings para cada ocorrência da palavra \"banco\"\n",
    "    for token in doc:\n",
    "        if token.text.lower() == \"banco\":\n",
    "            contexto = doc[max(0, token.i-3):min(len(doc), token.i+4)]\n",
    "            print(f\"Contexto: {contexto}\")\n",
    "            print(f\"Vetor (10 primeiras dimensões): {token.vector[:10]}\")\n",
    "            print(f\"Dimensão do vetor: {len(token.vector)}\")\n",
    "            print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar spaCy: {e}\")\n",
    "    print(\"Talvez seja necessário instalar os modelos com:\")\n",
    "    print(\"python -m spacy download pt_core_news_sm\")\n",
    "    print(\"ou\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Treinar modelo FastText\n",
    "modelo_fasttext = FastText(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# Testar com palavras que não estão no corpus\n",
    "palavras_teste = [\"filmagem\", \"cinematográfico\", \"atuações\"]\n",
    "\n",
    "print(\"\\nEmbeddings para palavras fora do vocabulário (FastText):\")\n",
    "for palavra in palavras_teste:\n",
    "    # Verificar se a palavra está no vocabulário original\n",
    "    no_vocab = palavra in model.wv\n",
    "    # Obter vetor do FastText (funciona mesmo para palavras fora do vocabulário)\n",
    "    vetor = modelo_fasttext.wv[palavra]\n",
    "    print(f\"'{palavra}' (no vocabulário: {no_vocab})\")\n",
    "    print(f\"Primeiras 5 dimensões do vetor: {vetor[:5]}\")\n",
    "\n",
    "    # Encontrar palavras similares\n",
    "    similares = modelo_fasttext.wv.most_similar(palavra, topn=3)\n",
    "    print(f\"Palavras similares a '{palavra}':\")\n",
    "    for p, sim in similares:\n",
    "        print(f\"  {p}: {sim:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similaridade_documentos(doc1, doc2, modelo):\n",
    "    \"\"\"Calcula a similaridade entre dois documentos usando embeddings\"\"\"\n",
    "    vetor1 = texto_para_vetor(doc1, modelo)\n",
    "    vetor2 = texto_para_vetor(doc2, modelo)\n",
    "\n",
    "    # Calcular similaridade do cosseno\n",
    "    # similaridade = 1 - distância do cosseno\n",
    "    similaridade = np.dot(vetor1, vetor2) / (np.linalg.norm(vetor1) * np.linalg.norm(vetor2))\n",
    "    return similaridade\n",
    "\n",
    "# Exercício: Calcule a similaridade entre os documentos abaixo\n",
    "documento1 = \"O filme tem uma história envolvente e atuações convincentes\"\n",
    "documento2 = \"A narrativa do filme é cativante e os atores são excelentes\"\n",
    "documento3 = \"O restaurante tem comida deliciosa e preços acessíveis\"\n",
    "\n",
    "# Calcular similaridades (implemente sua solução)\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_topico(texto, palavras_chave_por_topico, modelo):\n",
    "    \"\"\"\n",
    "    Detecta o tópico mais provável para um texto com base na similaridade de embeddings.\n",
    "\n",
    "    Args:\n",
    "        texto: Texto a ser classificado\n",
    "        palavras_chave_por_topico: Dicionário {tópico: [palavras_chave]}\n",
    "        modelo: Modelo de embeddings\n",
    "    \"\"\"\n",
    "    # Vetorizar o texto\n",
    "    vetor_texto = texto_para_vetor(texto, modelo)\n",
    "\n",
    "    # Calcular similaridade média com cada conjunto de palavras-chave\n",
    "    similaridades_topicos = {}\n",
    "\n",
    "    # Implementar cálculo de similaridade média entre o texto e as palavras-chave\n",
    "    # ...\n",
    "\n",
    "    # Retornar o tópico com maior similaridade\n",
    "    # ...\n",
    "\n",
    "# Definir palavras-chave por tópico\n",
    "topicos = {\n",
    "    \"Cinema\": [\"filme\", \"cinema\", \"ator\", \"diretor\", \"roteiro\"],\n",
    "    \"Tecnologia\": [\"computador\", \"algoritmo\", \"software\", \"programação\", \"tecnologia\"],\n",
    "    \"Esporte\": [\"futebol\", \"atleta\", \"equipe\", \"competição\", \"treino\"]\n",
    "}\n",
    "\n",
    "# Textos para classificar\n",
    "textos_para_classificar = [\n",
    "    \"O novo filme do diretor ganhou vários prêmios no festival\",\n",
    "    \"A empresa lançou um software de inteligência artificial\",\n",
    "    \"O time conquistou o campeonato após uma temporada difícil\"\n",
    "]\n",
    "\n",
    "# Classificar textos (implemente sua solução)\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
